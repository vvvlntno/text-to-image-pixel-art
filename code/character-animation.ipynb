{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHHC2k0tADqH"
      },
      "source": [
        "## Install required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIrgth7sqFML"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers==0.26.3\n",
        "!pip install transformers scipy ftfy accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xSKWBKFPArKS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "p:\\Miniconda\\envs\\text-to-image-pixel-art\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]\n",
            "p:\\Miniconda\\envs\\text-to-image-pixel-art\\lib\\site-packages\\diffusers\\utils\\outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "p:\\Miniconda\\envs\\text-to-image-pixel-art\\lib\\site-packages\\diffusers\\utils\\outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from diffusers import DiffusionPipeline, LCMScheduler\n",
        "from PIL import Image, ImageFilter\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIYohH-cAdU4"
      },
      "source": [
        "## Create diffusion pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0S-uIOp_6gdw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "p:\\Miniconda\\envs\\text-to-image-pixel-art\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "p:\\Miniconda\\envs\\text-to-image-pixel-art\\lib\\site-packages\\diffusers\\utils\\outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "p:\\Miniconda\\envs\\text-to-image-pixel-art\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Fetching 19 files:  32%|███▏      | 6/19 [10:21<27:47, 128.28s/it]"
          ]
        }
      ],
      "source": [
        "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icQ-QSZ0yp-K"
      },
      "outputs": [],
      "source": [
        "pipe.load_lora_weights(\"nerijs/pixel-art-xl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YRcPxJP8QB6"
      },
      "outputs": [],
      "source": [
        "pipe.to(device=\"cuda\", dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ESdgUpzApaA"
      },
      "source": [
        "## Create prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMa5tTiYAtSm"
      },
      "outputs": [],
      "source": [
        "prompt1 = \"One woman as a mage and a red robe displayed from the front showing the whole body in the middle of the screen with a clear white background, hands next to body\"\n",
        "prompt2 = \"One woman as a mage and a red robe displayed from the front showing the whole body in the middle of the screen with a clear white background, hands in the air\"\n",
        "negative_prompt = \"blurry, photorealistic, 3d render, realistic, multiple characters, shadow, detailed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuppCuk1TDw9"
      },
      "source": [
        "## Create seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl2cIClGSadV"
      },
      "outputs": [],
      "source": [
        "# 4382350725\n",
        "# 4382342934\n",
        "# 4382339547\n",
        "# 4382337597\n",
        "# 438233954\n",
        "# 4382356118\n",
        "\n",
        "base_seed = 4382339547\n",
        "seed_offset = random.randint(0, 20000)\n",
        "\n",
        "seed = base_seed + seed_offset\n",
        "\n",
        "print(f\"seed {seed}\")\n",
        "images = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx-0R4rqA3sZ"
      },
      "source": [
        "## Generate Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEErJFjlrSWS"
      },
      "outputs": [],
      "source": [
        "image1 = pipe(\n",
        "        prompt=prompt1,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_inference_steps=20,\n",
        "        guidance_scale=1.5,\n",
        "        generator=torch.Generator().manual_seed(seed)\n",
        "    ).images[0]\n",
        "\n",
        "images.append(image1)\n",
        "\n",
        "image1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFIRrf5hn09S"
      },
      "outputs": [],
      "source": [
        "for index, image in enumerate(images):\n",
        "  images[index] = image.resize((128, 128), Image.NEAREST).filter(ImageFilter.BoxBlur(radius=0))\n",
        "\n",
        "images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3Fy3IMWA9rN"
      },
      "source": [
        "## Handle character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef-3XMwh9aBM"
      },
      "outputs": [],
      "source": [
        "def color_within_tolerance(color1, color2, tolerance):\n",
        "    for c1, c2 in zip(color1[:3], color2[:3]):\n",
        "        if abs(c1 - c2) > tolerance:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def get_bounding_box(image, tolerance=10):\n",
        "    # Get background color (color of the top-left pixel)\n",
        "    background_color = image.getpixel((0, 0))\n",
        "\n",
        "    # Initialize bounding box coordinates\n",
        "    leftmost = image.width - 1\n",
        "    rightmost = 0\n",
        "    topmost = image.height - 1\n",
        "    bottommost = 0\n",
        "\n",
        "    # Iterate through every pixel to find the bounding box\n",
        "    for y in range(image.height):\n",
        "        for x in range(image.width):\n",
        "            pixel = image.getpixel((x, y))\n",
        "            if not color_within_tolerance(pixel, background_color, tolerance):\n",
        "                # Update leftmost, rightmost, topmost, and bottommost coordinates\n",
        "                leftmost = min(leftmost, x)\n",
        "                rightmost = max(rightmost, x)\n",
        "                topmost = min(topmost, y)\n",
        "                bottommost = max(bottommost, y)\n",
        "\n",
        "    # Return the bounding box coordinates\n",
        "    return (leftmost, topmost), (rightmost, bottommost)\n",
        "\n",
        "def crop_character(image):\n",
        "    # Get bounding box coordinates\n",
        "    top_left_point, bottom_right_point = get_bounding_box(image)\n",
        "    # print(\"Bounding box coordinates:\", (top_left_point, bottom_right_point))\n",
        "\n",
        "    # Crop the character using the bounding box\n",
        "    character_image = image.crop((*top_left_point, *bottom_right_point))\n",
        "\n",
        "    # Return the cropped character image\n",
        "    return character_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emWX-IvM3Cml"
      },
      "outputs": [],
      "source": [
        "for index, image in enumerate(images):\n",
        "  images[index] = crop_character(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5NbSekGBUHP"
      },
      "source": [
        "## Downscale to 32x32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qj8RGg6xBTSu"
      },
      "outputs": [],
      "source": [
        "def resize_to_any(pimage, size=32):\n",
        "  img = pimage.resize((size, size), Image.NEAREST)\n",
        "  img = img.filter(ImageFilter.BoxBlur(radius=0))\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arFrqfDv7I4r"
      },
      "outputs": [],
      "source": [
        "# cropped_image for character\n",
        "# image for background\n",
        "resized_image0 = resize_to_any(images[0], 32)\n",
        "resized_image0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2MaPBs5S8iv"
      },
      "outputs": [],
      "source": [
        "resized_image1 = resize_to_any(images[1], 32)\n",
        "resized_image1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
